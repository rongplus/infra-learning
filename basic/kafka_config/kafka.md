Course Overview
[Autogenerated] Hi, everyone. My name is Pablo Kordic and welcome to my course deploying pickup truck. Laster. I am currently a senior data engineer. It's far fetched dot com Apache Kafka is the messaging platform that, after its inception to the enterprise ICTY, world by storm and reportedly it's not deployed in more than 1/3 of Fortune 500 companies. Some of the major topics that we will cover include the reasons to use Kafka or message brokers in general architecture of a Kafka deployment and systems built around IT. Production and consumption off messages in a reliable way, insulation off Africa and high availability, and told the Laurent's of a Kafka plaster. By the end of this course, you will understand how Kafka fits into modern I t infrastructure, and you will have the fundamental skills needed to deploy a captor cluster and build applications around eight before beginning the course. You should be familiar with basics off any mainstream programming language, so that you can follow the simple code snippets in the demos, which are written in Java. I hope you'll join me on this journey to learn Apache Kafka with the deploying kick, aka Cluster course at Plural Side

Message Brokers
Introduction
[Autogenerated] Hi. Welcome to the course on Deploying Kickoff Cluster. This course will be a smooth and practical guide of the universe of Apache Kafka, a technology that change a lot of things on the enterprise architecture landscape. I hope you will learn, load and understand why this was the case. We will start by discussing, in general the category of products that Kafka belongs to, namely message brokers. Later, we will see what the core components of Kafka are and how interaction with Kafka looks like This will give you a great start to study Hulk of cooperates in a cluster guaranteeing high availability and fold stole arounds. Next, after knowing quote Kafka does and how it does it, we will see these steps required to install Kafka and its dependencies on the clean machines throughout the course. We'll also discuss some useful additions to the Kafka ecosystem. Before we dive deep into Kafka usage and deployment, I would like to first make sure that you have a good understanding of the problem that it solves and of its place in a modern enterprise, a little spoiler here. It often lies at the heart of digital operations and to put things in context instead of talking about calf kind. Part particular, I will. For now talk about message brokers in general message brokers for my family. Off software products off which calf guys, a prominent member, they come in different flavors may solve slightly different problems, but fundamentally they do the same thing. They receive messages from applications, keep them for some time and make them available to other applications. In this course, you will learn a lot about how messages are created, handled by a broker and consumed by clients. But before we look closer into death, let's answer a fundamental question. Why would you need a message broker? To understand that, let's think how computer systems usually communicate. Take, for instance, the Web HTTP requests from Browsers Server We have two parties here. One of them server, is actively listening, while the other browser may initiate a connection by issuing a request. The server, if it's ready and healthy, will return the response. These requests Response exchange may continue for as long as it is necessary. The example of a Web request from the browser. It's one that's very easy to imagine, but very often, and in case of bigger size, almost always the server. In order to fulfill the request, we'll talk to other servers within the company's network or third party services. These communication also quite often follows this request response pattern through rest AP eyes or protocols like soap. To have a more concrete example, we will look at a hypothetical architecture of right hailing service and analyzes subset of interactions between components off such platform. The flow usually stars when the user opens the up and indicates that they want to go from Point A to point B. These action triggers the request to platform servers more specifically to the component that we can call booking service. These can be a physical server cluster of servers, a container we don't know. It doesn't matter for us how it's deployed. What is important is that it is responsible for managing three bookings. Assuming that the trip intent request contains start and end points for the tree, the booking service now needs to get information about the exact drought so that it can know the distance and thus be able to show the quote to the user. For these, it may have to call some other possibly third party routing service, which will respond with complete route between A and B. In our scenario, the route information is all that is needed to call another service and receive a quote. The court service probably uses more information than merrily he route in order to minutes strings like surge pricing. But from the booking service perspective, the route is the only thing necessary. Giving the quote. If the service can push it back to the up, where is it will be presented to the user for acceptance if they wish to take the right Likely confirm and an except quote request is sent back to the service. Now it's time to look for a driver. Therefore, the booking service needs to pass this information to some other service. Let's call it this part service so says the driver can be found in the meantime. This information is also useful for the quote service so it can manage the price multipliers. Depending on the situation, it is important to observe that in this architecture, the booking service needs to know about all the services interested in receiving the court accepted event. Imagine the new service for data analyst is created, and it also needs to receive quote accepted events in such case, and you coat would have to be added to the booking service. So it also called the analytic service with the same information. What is also interesting here is the fact that none of these three services interested in the court accepted information has to immediately reply back to the booking service. Looking for a driver may take a while, so the booking service will rather receive separate requests from the Despair service when the driver is found. Not as the response to this quote accepted. Call. These is even more visible in case off the court and analytic services where there is nothing that the booking service would want back from them. And yet it must still know about the services and be able to handle situations where this service is misbehave, for example, it needs to handle rich rice of the request. In such case, developers off the booking service would probably prefer to have a fire and forget mechanism. What if there was a kind of a message board where they could just write the quote accepted message and whoever would be interested in outer rise would go and read it from there. In that case, as long as the board is available, the booking service does not have to care about services that it does not depend on. The same thing would happen when the driver is dispatched. This much service with put car dispatched message on another board and the booking service could freed it from there. If anyone else is interested, no problem. Just observe the board. I hope you start to say that such architecture, we've a synchronous message passing leads to loser coupling of services, which is often a very desired property. Of course, it doesn't mean that requests response pattern becomes obsolete, for example, to quickly get throughout from the routing service. Probably a direct synchronous Cole is the best solution. Nonetheless, if you apply request response communication everywhere, you may soon end up with an unwieldy spaghetti of dependencies between services. When technical teams are tasked with supporting increasing number of business cases, it is easy to accumulate massive technical debt that later hinders agility and ability to innovate. The case I presented with the right hailing service. It's really simple, as in real life, the number of services involved would probably go into hundreds. Just try to imagine the number of direct connections between services without this message board design. The bottom line here is that such message oriented communication has been applied in enterprises for a long time and has proven itself to be a great choice for simplifying certain architectures. These abstract boards I've put on the diagram are in really ICT infrastructure serves by nothing Gilles than the message broker being a single one or multiple of them. And in a moment we will take a closer look at how brokers operate and what patterns are flight in systems that use brokers.

Message Broker Patterns
[Autogenerated] just to remind a broker is a piece of middle where that accept messages from some applications and makes these messages available to other applications. Applications on both sides can be named differently, but in this course will stick to the Kafka parlance and call them producers and consumers. So they flew is the following producer writes a message to the broker and consumer subscribes to one of these boards in order to be notified when new messages arrive. It's easy. Is that what can these messages contain? Practically anything for the broker. There are just mostly bites without any special meaning. And it's after producers and consumers to agree on the protocol of how to handle these bites. We'll look at how such an agreement can be guaranteed later in the course. But now let's assume this is a non issue and focus on how messages can flow. And they can flow slightly differently depending on the use case, which is reflected in two main patterns that can be applied in message oriented systems. These are published subscribe Abbreviated, This pop sub and message queue. Let me explain fares the publish subscribe butter and as it is easy to discuss using our right hailing example. The main idea off the publish subscribe pattern is the completely coupling of producers and consumers. It means that the producer does not send the message with any specific consumers a target and interested consumers create subscriptions in order to get messages that they are interested in exactly like in the right, hailing example where the court accepted message was put on the board by the booking service without any idea who would read that. So the number of consumers and the number of times a single messages consumed is practically unconstrained. What is important, the number of producers is also arbitrary. If the application, instead of sending your request, would send a trip intent message to the broker, each application will become a producer on its own. One thing worth noting is that we have to somehow manage different types of messages within a single broker. If we have quote accepted and Cardiss parts messages flowing, subscribers need to have the possibility to listen on Lee two messages. These are off interest for them In the abstract diagram, I have put two boards in Kafka. They're represented three topics. Producer always sends a message to a specific topic, and consumers give the freedom to subscribe to the topics off their choice. Topic is a caftan A for the concert. Some other brokers use it some not, but is, I've told before we'll stick to the Kafka parents in this course, let's look at the message queue pattern now in which we will use the broker more like a set off real life mailboxes in this scenario, for these airs published messages to the mailbox of choice. But this time the intended recipient off each message is known, and the messages are written exclusively by eat. So the mailbox analogy works well here. A message can be picked on Lee ones and by its owner. This mailbox must be additionally accused so that messages flow in the first in first out fashion. This pattern is very often applied to task use, where the producer has some work to do and knows that on the other side of acute there is a worker or a pool of workers dedicated for these task. It puts the messages in this case task descriptions in the queue, with the clear intent of having the task processed only once this is practically the same thing. Is worker threats or worker processes on a single machine just over the network. And we have rocker in the middle? No, that multiple producers are also allowed for single May looks to make things clearer. Let's imagine the video hosting side. There is the server to which users can upload their videos, but before these videos are visible on the website, there is some additional processing that needs to be done. This is a CPU intensive tasks. So instead of running on the same machine that it's serving the user interface, it should be offloaded to pull of dedicated servers. The communication channel between the server is nothing _____ and the message queue. Whenever a new video is uploaded, the server will and queue it common for the pool of workers so they can pick it up. And prose is the video. No, that here we have a queue of tasks and the recipient is clearly defined. This command is not intended to be read by anyone else. Also, only one worker should process it, since this is an expensive task and there is no point in doing it twice. What is important in that scenario is that when the intended consumers confirms it has processed the message, it can be removed immediately in our system. One form of confirmation of processing could be a message to the server that the video is done. Nonetheless, in a message queues. The consumers are also responsible for sending back to the broker what is called an acknowledgement. This is a form of informing. The effect of the message will not be lost if the broker decides to remove it to recover. The main difference between the two patterns is the different expectation about the consumers in pops up. It is not known what will be done with the messages and how many consumers will read them. In acute, there is a known set off consumers from which only one should pick a message. In the real world of message rookers, there is no clear distinction between Pap SAP systems and Queuing system is. Most of them support both patterns, even if one of them feels more natural with given technology to citizen more detail. In the next video, we will talk more specifically about Kafka and how it compares with other brokers

Kafka
[Autogenerated] So far, I've talked about message brokers. We've had looking get an in party Glor technology. But this course is actually about Kafka. So why should you be interested in using Kafka at your organization and what makes it stand out from other brokers? First of all, it may be useful to give some background on its inception. Kafka was started some years ago at Lengthen, and one of the initial use cases was reliable. Processing off large number off user activity events before they work on Kafka was started after brokers were evaluated. However, several shortcomings were identified in all of them. As a result, a team of engineers decided to create a new solution that would improve over them. This gave birth to Kafka with its several highly desired properties which are scalability and high availability out of the box. Accommodating increasing needs in terms of volume of data and availability guarantees does not require any fundamental changes to your deployment compact message format for high throughput, low late INSEE and optimum disk usage and adjustable on this data retention, you can keep the data regardless off it already being credit by all active consumers and your ability to start. Historical data is limited only by the disc space. All those things combined in one product made CAFTA quite a special offering when it first appeared and contributed to its popularity. As we progress through the course, you will be able to see how Kafka works on lower level and understand how each of these properties is backed by design. This will let you take full control of them. In the previous clip, I promise to talk more about how Kafka supports, publish, subscribe and message queue patterns and compare it with some other brokers. I cannot explain everything in death at this point, since I haven't told practically anything about Kafka model, but enough has been presented to plant a seed. We'll also come back to these two patterns and reflect on how Kafka supports them. As we move on for the course, I just need to put one more note here. These patterns are just patterns, and most of the popular broker implementations let me apply both in different ways and cannot be categories simply is supporting either one or the other. The difference is always in how you use them and how your architecture applications around that. Nonetheless, if we think in terms of those patterns Kafka, it's more of a publish subscribe product, though it is still possible to use it as a cue. So why do I say that calf guys, more of a publish subscribe? This comes from the fact that Kafka handles messages in a different way from traditional pre Kafka messaging systems like rapid, MQ or Active in Q. The letter started out as message queues and keep messages literally in queues. That means that messages flow in the first in first out fashion and out in the case means that when the data is read by consumer and acknowledged it could be removed from the Q is each message has its own life cycle. Kafka instead does not use a Q underneath but a log, an order sequence off elements one off. The reasons for that is the fact that calf casters, every message on disk and this log approach is well suited for the scenario. And while conceptually this may seem not so different from the cue, the implementation is very different. The main result of these is the lack of a pair, a message lifecycle management when Kafka removes messages. It always happens in batches, and it's completely detached from consumers activity. Instead of checking which message has been consumed, Africa keeps track off were in the Locked the Consul Marie's. And it has no impact on message retention, which is governed by independent settings. One of the great benefits of this design is the ability to keep messages on this forest long as you want. So it is possible to as new consumers long after the message was first consumed. Unlike in a typical message queue where the message would be long gone, this gives you some nice guarantees. For example, imagine that you deploy in new AB that consumes from Kafka and after 12 hours to realize that there was a back and you need to re process all the messages from the past 12 hours because Kafka keeps them on this. This kind off rewinding is not an issue. On the other hand, if you used rabbit them cure active MQ. You would have to make sure that you produce messages again in the right order. There are also systems that sits somewhere in between. For example, Google Cloud pops up that lets you replay messages, but the retention period is strictly limited to one week. To some, there are many things that each rocker can implement differently. And while no less an understanding of the generate pops up and queue patterns is important, it is crucial to understand well, any broker technology before you decide to use it. Having said that, it's Kavika improved over many shortcomings off older technologies. It is quite flexible, and it is a very good choice for many use cases. The number of high profile companies using Kafka is an important part of the infrastructure should speak for itself.

Summary
[Autogenerated] Let's recap what we have discussed in this module. First, we so why a message broker might help you deploy more maintainable architecture. Then we compared to main patterns that employ message brokers, namely publish, subscribe and message queue. We so many differences and concluded that these are generate patterns, not strictly defined categories. Off message broker implementations. Then I provided a 10,000 foot view of Kafka's message management mechanism and explain how it differs from other popular technologies like Rabbit. Thank you are active. MQ. I know that many things may not be crystal clear at this moment, but don't worry. Well, no. Dive deep into Kafka, go through different implementation details and lots of court exercises in which will implement parts of our hypothetical right. Hailing surveys, this will let the knowledge sink in and make you well versed. Calf guy engineer

High-level Kafka Architecture
Introduction
[Autogenerated] after a more theoretical introductory module, this one will be much more practical. We will focus on understanding cuff Karki texture. You will see what constitutes a minimal Kafka deployment, how messages travel from producers to consumers. Learn more about core concepts of topics and partitions and understand what are the most important settings that you might need to tweak in the coating parts. You will see how to run the minimal Kafka cluster in your local environment, how to interact with it, using the admin client and how to produce and consume messages. We will not talk about high availability and scaling a Kafka cluster yet. But since Kafka was built with those concepts in mind, knowledge gained in this module will be enough to afford. Leslie. Prepare your applications for the possibility of increasing glowed. So far, I have referred to message brokers this abstract entities. But now the time has come to fully focused on Kafka. To start, you need to understand what exactly constitutes a Kafka deployment imagining to create a proof of concept of a system using Kafka. Where do you start? How do you get a development environment running for days? You need to pieces, at least a single calf broker and at least a single zookeeper note. In production, you will always want to run more of age, but we'll talk about this later in the course to test things and learn Kafka concepts. Single broker and zookeeper is enough. You may be wondering why Kafka broker is not enough. And what is the keeper doing here? Or even you may have never heard about zookeeper, and they're thinking what that is in the natural zookeeper is a coordination service, and it is one off several tools that are indispensable in high availability distributed systems in such systems. To be fault tolerant, you need to have multiple instances of your service running and for debt. You need to be able to have a consistent view of the world, which requires some kind of consensus between the servers, and that is what zookeeper provides sayings. Calf guys focused on high availability from the start. It does not work without zookeeper. It is important to know that since the first release, CAFTA has been relying Cholestin Lesson zookeeper, and while at the time of production of this course it still uses the Keeper, it is possible that at some point it will manage everything using Kafka brokers only. But for now, zookeeper is needed. You will learn more about the keeper role when we talk about availability and fault tolerance later in the course. For now, please accept that it must be deployed along Kafka. Having zookeeper, we then need the corpus off. Kafka, a broker broker, is the several that manages topics, messages, persistence and lifetime and to which producers and consumers connect. This is the heart off any Kafka deployment, and almost everything that happens in Kafka is down on the broker. There can and almost always are multiple brokers in a single Kafka deployment, but we will get to death in the module about for tolerance.

Interacting with Kafka Broker
[Autogenerated] we will take a top down approach, and for a moment we will assume that we have captains to keep her up and running in order to focus on learning how we can interact with a broker to guide you through different Kafka features. I think it makes sense to continue with our ride, hailing up example and to try to put parts of this system to life. First of all, we need a topic to keep court accepted messages so we can connect applications from both sides later on. By the way I will know represents topics not by boards but by this look with individual messages is now there is no need to be so abstract, and the letter represents calf cut off its better. Instead of creating a quote accepted topic, I suggest we keep some flexibility here and call the topic court feedback so that information about declined quotes can also be published to this topic. This is what we want to have. But after starting the Catholic a broker, the topic will obviously not be there and needs to be created. Tough guy lost automatic creation of topics, but let's take a principal approach here and create the topic. Mannelly. To create the topic, we need to send a request to the running Kafka broker. So how do you communicate with Kafka? Contrary to what might be the first guest here, Kafka does not use any kind of rest 80 i that requires http request or any other standard application layer protocol. To keep the communication fast and compact. Kafka uses a custom binary protocol, So in order to be able to communicate with Kafka broker, you need a client that understands this protocol. Luckily, such clients are available for many programming languages as well as for come online For the most popular languages, you will find three core components in each library. Consumer producer and admin for topic creation, which falls into the category of administrative tasks. We need the last one having the admin client in your favourite language. You can issue a request to create topic and that state. We will look at complete code examples in a moment, but to give you a taste of how you interact with Kafka, let's take a look on how typical code to create topics. Looks like just to clarify all the examples I present in discourse will lean Java. This is firstly because Kafka itself, it's written mainly in Java, so it is the most natural choice. And secondly, Java syntax is quite verbose, which I find generally nice for demo purposes. In any case, if you prefer other languages, you should have no problem adapting things I present with Java to other languages is all the clients for the same naming and butters. If you are interested in what exactly languages have Kafka claimed and which features they support, you can always check the website off. Khan Fluent. This is the company's per heading Kafka Development and started by its creators. Let's look at some Java code then. First thing is to create the abdomen object which will allow us to perform administrative tasks we need to pass configuration properties is an argument at least address off a single calf broker. We will always work, luckily, so it is local host and we will use the default Port 1992. Having this admin will let us create new topics. For that. We need one more thing. A new topic object that it's used to describe a topping before we attempt to create it. These constructor takes topic name as the first argument, plus some additional ones that will discussing the demo. But in any case, they are irrelevant at this point. Then the last step is to call creates topics, which is a method off the army. An object know that it takes the least off new topic objects. So even if we want to create a single one, we need to wrap it in the least visual return. An object of Kafka future type future implements a standard Java future interface. Now, if you are familiar with futures, great. If not, don't worry, as we will not do anything complicated with them. Here. Future simply represents an operations that runs in background and may complete with a result or exception, because create topics, returns the future. If we want our main threats to wait for the results, we need to call the get method. Otherwise, the tread with advanced without waiting for the future completion. Additionally, I will also add a call toe all method, which means that the operation will complete on Lee after all the topics have been created successfully. In our case, it's one topic, but in the future, we will create more of them. So just so you get used to it. I included here this essentially all that we need to create a topic on the calf broker. In a moment you will see the complete codes needed to achieve this task.

Running Kafka
[Autogenerated] as promised, The time has come to start our hands on exercises. I will not show complicated things about calf kind. This clip I will treat it as a warm up around so you can get used to the environment, understand how to bring Kafka up. And from the examples, I also encourage you to carefully read the set up instructions attached to the course files. I will be using intelligent idea community integrated development environments to present the code examples. But I will run everything from the common line so you can call it using your favorite it later and get the same results. So let's get started. You work for a ride hailing company and you are tasked with creating proof of concept for the system. Using Kafka is the current architecture is becoming in maintainable. What did you even stars as previously mentioned? To do anything with Kafka, we need to run it least one zookeeper note and one Kafka broker. We will run both locally using docker containers. The nice thing about Docker is that containers for both zookeeper and for Kafka are available from confluence and ready to use. We can find them on the Doctor Hub, a popular place to share container images. Okay, the individual containers air there, but it would be nice to have the startup off, both coordinated and automated. For that, we will use docker composed throughout the course. Dr. Composed lets you orchestrate individual containers into a complete system. To do this, you need to have a docker compose IAM, a file that describes which services we want to deploy. Let's look at the contents of such file for this module created to run a minimal Kafka set up on the top level. This file has two entries. Version of the Doctor Composed File Schema and Services Services section, as the name implies, describes the services we want to orchestrate. In our case, we want to have one's a keeper and one calf broker, and these are exactly the two interest provided. Note that the names of the services are arbitrary. I could as well use a, B, C and X y Z, but I'm sure the least confusing is to have simply zookeeper and Kafka under the zookeeper section. The first perimeters image. It is extremely important, as it specifies, which docker image to use for this service this name corresponds to the one on Docker Hap, and the image will be pulled from there. Then comes the Environment section. Here you can provide arbitrary number of environment variables as it will be set on the container. The containers from Con Fluent requires some variables to be provided. Otherwise, there will not start for zookeeper. There is single menagerie variable Zookeeper clients board. It defines on which board zookeeper will listen for client connection. And this is the board on which Kafka brokers will connect the zookeeper. Now let's see what we have for Kafka fares that there is the image for a Kavika rocker specified in the same way It's for a zookeeper. Next, Andrew here depends on its new it informs Dr Composed that in order to start Kafka, we need to have zookeeper up and running first. We also have a bit more extensive environment section, so let's go variable via variable. We start with Kafka Zookeeper Connect, which is used to tell Kafka where to connect. Zookeeper Board is the 21 81 that we have defined just before for zookeeper, but what's interesting is the host name. These is one of the cool features of docker compose that the service names are recognized. Host names within the same docker compose network know that this will not work outside of Docker. For example, are Jeffer clients running on the host machine will not be able to resolve the simple CAFTA host name. So how can we connect to Kafka from our applications? This relies on Kafka advertised listeners and Port ma pings. Let's see first the Cuffed advertised Leasing. There's variable. These scripts work after we listen for new connections and held. This information is exposed to others. That is why the advertising name. We have plain text and plain text host entries. The first one is intended to be used by the brokers within the same network. That's why the CAFTA host name, while the second is for the clients on the host machine. That's why the local holes, but it's not enough to simply specify ports. It is because when you have multiple brokers, the server that the contact 1st may not be the one that you ultimately connect to imagine to brokers, which we've advertised listeners and the client that can resolve both coast names but only knows the first one and wishes to connect to the cluster. In this case, it will first add city broker where to connect. The cluster will then decide which is the right broker to connect, and the contacted broker will return the address that the target broker advertises. Client will take it and connect to these. That's why this host name that can be resolved by the client must be provided again. Multiple brokers will be covered later in the course. For now, you can forget about it. I just needed to explain why we need to provide such settings. The less variable is for setting protocols for the listeners. It maps names of the listeners to security protocols For the most. I will use plain text, which is not securing an away, but be aware that it it's used often times in production deployments. Please may seem strange, but usually in production. Calf guys deployed in strictly controlled networks, so there may be no need for this extra layer of security. Novus, depending on the business security team in the company, may require having authenticated and authorized connections to Kafka. I will not cover dead in this course because I decided there are more important things to teach. But be aware that such option exists the last thing we need to define and the second piece that is required for clients outside of the doctor network to be able to connect is the port mapping by default. You cannot access anything running inside a docker compose network from your local machine. We definitely want to be able to do that. Therefore, we have to mom the ports from the doctor host machine to the Doctor Service. So it means take work 1992 off the Kafka Service and expose it as 1992 on the host machine. Now, whenever we connect the local host 1992 we will be taken to Cathcart 1992 inside the Doctor Network. Having this complete environment defined, we can now start our services. To do this, we need to have the doctor _____ running. Go to the folder with the Docker, compose IAM a file and run docker, compose up comment. This will prepare the environment and started service is defined in the file. In the output of the containers, you can see a lot of things as these containers from con fluent have very relaxed logging settings. We will take care of that later. But now the logging will stop at some point, and it means that Kafka has successfully started. It's time to connect to the running broker now.

Connecting to Kafka
[Autogenerated] the main example file contents most of the code, But I love the most important parts that I want you to focus on is to do notes. We will be feeling these missing pieces in the court examples one by one throughout the course. If you feel like trying things yourself first, you're more than welcome to pose and to try to fill the gaps yourself before you watch what I do. We have a simple main method here with an admin client that connects to caffe con local host 1992. Then there are two Coast, the print Ulta Bix methods This method is below, and its task is to get all topics from the running Kavika cluster using Adam pastors the argument and prince their names. For the moment, it's not doing kit because the least off topic names is in fact, at least with single empty string. So to get it working, we need to actually get the list of topics, and this is a good exercise. To get started with the admin client, I will remove this fake topic least and create a real one by calling the least topics methods this itself gives us complete information about the topics. But as we are only interested in names, we can call names methods. As this returns the Kafka future in order to synchronously wait for the result, we need to add a call to get. Let's see the effect of these in a new common line session. You can run the build common to execute the main method of our current file. First thing that you see in the output are Kafka local interests related to the admin client we have just created below. You can see topic names printed two times and because we did nothing in between, the outward is the same, a single topping that is created by default. Now we can feel the second missing piece in the file to create the topic. We will do what was presented on the slight in previous clips, starting with new topic object in the constructor, we first past the topic name and then two arguments that I omitted on the slide. These are number of partitions and replication factor. None of these was discussed till now, so I will opt for the cluster defaults, bypassing empty values if you are not familiar with Java optionals, This optional empty is simply a safe way to indicate a non existent value. Instead of passing canoe, we will discuss partitions soon, and replication factor will be discussed in the module about high availability. Now we have the code that actually creates the topic. If we run the main method gain, we can verify that the topic was in fact created. Congratulations. You have started the minimal Kafka cluster and created your first Kafka topic. Now the last thing before finishing this _____ posession is to gracefully bring down the containers. To do that, you need to go to the council, where you run docker, compose and hit control C. This will graceful east of the containers after they are stuffed. We still need to run docker compose down comment. This will remove the containers and the doctor network. Why do we want to remove the containers In that way, no state will be preserved between exercises guaranteeing that we always start with a clean Kafka deployment

Partitions, Producers, and Consumers
[Autogenerated] after this warm up, I hope that you are ready for a deeper dive into the machinery of core parts of Kafka. In the previous exercise, he created a topic and provided empty value for the partitions. Argument. What is a partition? It is a logical and also a physical union into which topics are divided whenever a message is sent to Kafka. It always ends up in one of the partitions that keeps its portion of messages. Partitions are Dickie Toe, Kafka, scalability and high availability, and it is not possible to understand producers than consumers behavior without knowing it. The role of the partitions. I personally think that it is best to introduce this building blocks off Kafka all at once before looking at each of them in more detail. As this will also make clear why partitions are needed. Let's continue our work on the ride hailing service and take a closer look at the quote feedback flow. We have the booking service, producing messages and these parts service, reading them and handling car booking. A question that we need to ask ourselves is what the volume of data and tripled over services is. Let's say we are not the market leader but already a big company with many customers. 20 bookings accepted per second is a reasonable number. Assuming that the booking services able to handle this traffic, we will have it publishing twenties messages for seconds to Kafka. This is well below the number that could make Kafka broker sweat. So these 20 messages per second arrive ready to be processed at the other end. The this part service starts reading, and we find out that it can't keep up with that parade because it needs to do some expensive calculation and is only able to process 10 messages per second. This means that UN processed messages are being accumulated and customers are not getting responses in time. In that way, we will quickly end up with no customers and to save the company, we must handle the dispatching faster. The solution is to have more instances of the dispatch service consuming messages off course. They need to act in a competing consumers way so that each receives different messages. The idea is exactly like in the message Jupiter an example from the previous module. There was some dispatching work to be done, and we have a pool of workers ready to do the job. Do we precise? We need at least two instances of the service to keep up with the production rate of 20 messages per second. But how do we scale our consumption with Kafka? To understand how the scaling is achieved, we need to zoom in on mechanics of message consumption and see how consumers interact with the broker. First, let's consider a typical pre Kafka message queue, where a free worker picks first available message. These messages then unavailable for others. And if everything goes well, the workers sends an acknowledgement. The message, then these appears. In this scenario, you have a single que with many concurrent consumers, and they will not step on each other's faith. Such approach, however, is not performance enough for Hailo scenarios. Saints For each single message, the delivery and acknowledgement run trip is done. Also, all the consumers are hitting the same Q. Since Kafka was created to sustain enormous through boots, it followed a different path. In Kafka, we have a resistant log, and the messages are appended to it. Each message is assigned a unique incremental offset when a new consumer joints it can explicitly request rate from a specific offset. But let's agree that it starts from all this message to save network bandwidth and increased throughput, Kafka will almost always send messages in batch, say one in Still here. When the consumer is done with those, it will inform calf cables. That tough guy broker will take note of that and stories consumers offset indicating quarters. Consumer is in the log. That means that Kafka does not store in apparent message data to know if someone is already processing kids. Nor are the messages removed based on consumers activity. Combined with dynamic batch size, this makes competing consumer scenario within a single calf Kellogg infeasible. If other consumer starts consuming the same log, both will process all the messages. This is not what we wanted off course. Kafka would not be where it is today without support for the competing consumers pattern, and the solution to this is in partitions. Within each topic, Kafka maintains separate physical looks for each partition and store subsidy off messages In each. This means that different consumers can process messages from different partitions to increase troop food, and this is exactly what we wanted Let's assume that the quote feedback topic was created with three partitions. When we deploy our single instance of the dispersed service, it will be assigned all three partitions to consume, and it will not be able to keep up with throughput. So we decide to scale artist for service to two instances. What happens if the second instance council line in contact Kafka First Afghanis to know that this is another instance of the dispatch service that wants to cooperate with the first one? If we, for example, where to deploy analytics service Now we would like it to read all the messages, regardless of the dispersed service activity. To define which consumers are working together, capture is an obstruction off a consumer group. This means that our second instance off the service needs to indicate that it belongs to the same consumer group as the first one. You want to have a meaningful name for Consumer Group, and often the most reasonable approach is to give them the service name, so we would have a dispatch service consumer group. In our case when Kafka Machinery figures out that the second consumer is part of the same consumer group it will three year what is called partition rebalance. This will redistribute partitions between the consumers. If we had third, the instance of the service will have each consumer consuming from one partition, which should be more than enough for our throughput. This all happens dynamically. So if one consumer crashes, partitions will be re balance between the available ones. An interesting thing to consider is if we have a topic with three partitions and three consumers. What happens when a new consumer joins the consumer group? Well, in this case, nothing. Kevin's and the consumer will not be assigned. And importations tough guarantees that each partition is at the same time consumed by it. Most one consumer from a given group. So in our case, the forest consumer would be left idle. This is a very important thing to understand. Number of partitions is the factor that decides how much you can scale your consumption, and you should decide on this up front. It's adding partitions later in the process is not a straightforward task, particularly with some rough estimations. You can get the necessary number of partitions plus running up. It's usually okay. We will talk about how to choose the number of partitions later in the course

End-to-end Demo
[Autogenerated] Let's no try applying cold historian practice by producing messages and using to consumers in the same group to read them. How does he called for a typical consumer look like? First, we need to configure it. Bootstrap Server's configuration Property is nothing new. Group I D indicates the name of consumer group to join or create the last one value. This realize ER is specifying cadaver class that will take care of transforming crow bytes received from Kafka into a Java object so that your application can understand incoming messages. Next, we create actual consumer object. It is parameter raised by string is well for now, deal with messages that are simple strings. Next step is to subscribe to single or multiple topics. After that, you'll usually want to consume messages as they arrived. So we need to enter an infinite loop, which is called the Poll Loop. In every iteration, the consumer will pull Kafka for new messages and process them by default. This consumer will commit off this Kafka automatically. On the other side of the graphic a broker, we need a producer. Let's see how to bring one to life here. We also need some configuration properties. She realized her is in this case responsible for taking Java object that we want to produce and converting it. Throw bites. With this, we can create Kafka producer object Whenever you want to send a message, that producer record must be constructed specifying topic to which send the message and the message itself. The last step is to call Send my heart for each records you want to send to Kafka. Let's go back to the code editor and work on our first consumer and first producer and see ____ topics with multiple partitions. Behave. Before that, we will apply in minor tweak to our docker file. We need to provide another variable offsets topic replication factor. By default, it may be set to three, which will result in errors in operation with single broker. Don't worry about understanding it. Now we'll get there. We've containers up and running. Let's start by creating the court feedback topic again, this time explicitly setting number off partitions to two and replication factor to one. If we provide the number of partitions, we are expected to also provide the replication factor, and one will do since we don't care about replication for now. Note that I also added the line off code that closes the ad mean which will shut down, declined gracefully. When we are done throughout the course in files that don't change much between exercises, I may be adding some additional code if it does not change core functionality but introduces some good practices in the command line. We can around this and verify that the topic is created. Nothing special here now weakens to something new and set up a consumer that we will wait for messages from this topic. Here we have a minimal consumer implementation. Our program will take the consumer group name from the common line as the argument. Apart from the three properties shown on this slide, we also have a man that Ricky this realize their value, which is should ignore for now. We then subscribed to the topic and enter an infinite pole Luke to accept recourse. Our challenge here is to print each messages value partition and offset to achieve that. First, we need to get new records from Kafka. Therefore, I will add a call the bowl method that will return a set off records. We also need to have the time out parameter. It defines how long the client will wait for new records before advancing. If no new record arrives, we considered to one second, but our application is not doing much, so this is a relevant poll will return a set of messages over which we need to ease rate. Since this record object is often partition information attached to it, we can easily print it in the council along the message value. Let's run this now. In the output, we can see that the consumer joins the group, and since there are no other consumers nor offset stored, it was assigned both partitions. We have 20 Let's now produce some messages to see how consumer reacts to them. To do this, we need to create the producer object, which also, in addition to what we've seen on the slide, his Amanda Triki serialize her property. Let's create three messages and send them using unsurprisingly, these sand methods to make sure that the messages air immediately sent to the broker. It's useful to agriculture the flash method. Let's see how this looks from the consumer side, where you can see that all the messages were put in partition one and consumed by our only consumer. Let's produce few more rounds. At some point you will see that the messages were routed to different partition, with its own offsets and still consumed by our only consumer. Now let's see what happens if we start another consumer in the same consumer group. As I explained before his triggers, a group re balance in effect, the first consumers partitions, air revoked and re distributed among the consumers for both consumers, we can see that they are assigned a single partition and what is important you consumer gets the offsets that were committed by the first consumer, so it will start where the other left off. We cannot produce some more messages and verify that each consumer is processing its own partition and that the new consumer is not processing passed messages. And that's eight. These examples showed how we can use multiple partitions to apply the competing consumers pattern, I imagine, have Steelers of questions. Kafka is not a simple piece of technology and understanding Kit takes time and practice. Therefore, in the next module, we will focus on producers and consumers to get a deeper understanding of how they work. Let's do all the necessary clean up and get ready for a more exercise focused module. This module introduced quite a lot of concepts. You've learned how to start a minimal Kafka deployment, using darker and darker composed and how to create topics. Then I explained how tough these are made of partitions and how this affects consumers. You were also able to use Kafka and to end for the first time by producing messages and consuming them from the other side, applying the competing consumers pattern.

Producers and Consumers
Introduction and Record Keys
[Autogenerated] in the previous module. You learn some basics of Kafka architecture er and created your first end to end obligation. You also learn how to create producers and consumers. But to be honest, many things are still unknown. Who decides in with partition, the message ends up. How are partitions assigned to consumers? How do we guarantee that messages are not lost? How does the consumer commit officers behind the scenes? These are just some of the things that are the keys to using Kafka effectively. Therefore, in this module, I will uncover details of consumer and producer operation. You may find it a bit weird that the producer and consumer behaviors are essential to understanding Kafka in the end. For example, in the sequel date of its world, the client that you using your application is usually just a thin layer, and Onley issues Commons that offload the whole work to the database. In case of a sequel Query. Such clients simply takes it and sense to a server. In the world of Kafka, things are pretty different, and this client server relationship is more complex. Producers and consumers are an integral part of cuff cooperations, and some of the essential tests are actually their responsibilities, not off the CAFTA broker. That's why I will dedicate this model to producers and consumers. Let's start by looking closer at the producer code from the previous module. One thing that I left without explanation is the case. Realize er property as an issue. Realize er it is the code responsible for converting to our Java objects into row bites. But what does a key do? It is an optional part of a Kafka record that is primarily used for partitioning. In the previous example, we didn't use any key, but he can and usually will, at the key to producer records. So it is the producer's responsibility to row the message to the correct partition. By default, CAFTA uses hash based partitioning, which guarantees that records having the same keys will end up in the same partition. What could we use this keys in our booking service producer? One way would be to choose the user i d. For simplicity. Let's assume that we assign integers is ideas to our customers our records with within constructed like that. Using the three argument constructor, note that the key is often teacher type So we need to properly said the first diaper meter for the producer and the record as well as the key serialize er property. See that I m link away from writing serialization class names by hand to getting them directly from the class definitions, the previous one you may find in many Kafka tutorials. But I personally prefer this one, as I find it less error prone. No, that both interior and strength. She realized her classes are shift with the Kafka Library.

Demo: Partition Keys
[Autogenerated] Let's know, see how messages are sent to different partitions and verify her by specifying a key. We can give guarantees about messages. Partition assignment. The doctor composed the ammo file is practically the same as in the previous module. I have only had it two variables in order to have less outputs that we don't need printed to the console. After starting a fresh environment, I can create the booking feedback topic with two partitions. You think unchanged version of the killed from the previous module and go directly to our new producer implementation. The serial Isar classes are already changed. If you want to check for yourself, you can leave your verify that using wrong serialize er will throw an error. Here we have a brother, sir, with the right type perimeters. So now we can focus on feeling the missing code. The idea is to create three messages and print the record meta data to check the partitions. An important thing is the juice case, so that we will have to in one partition and one in the other. Let's assign the same key for two off them and add one for the other one due to the nature of hashing. I expect that given to partitions, consecutive in teachers will end up in different partitions unless capitalist something unexpected with those values. So let's put 1000 for the first insert a message and at one for the second message. Now, how can we checked? Which partitions are these messages assigned? The scent method returns in the object off record meta data type, corrupt in the future. These meta data contains the information about chosen partitions. So after each sand, we simply need to retrieve it and display to the council. Let's run. This is expected first and last messages are sent to the same partition, and the 2nd 1 is directed to the other one. I hope that this short demo has made clear what is the effect of specifying the key for a message.

Choosing the Right Key
[Autogenerated] you. So how a keyed the term is the partitions in which the messages and but it's important to think about what should be considered when choosing your key. You should always consider upfront how your messages will be consumed as each partition is that each moment consumed by it most one consumer. Therefore, if consumers do some aggregation by a data dimension, for example, compute the count off writes per customer. All messages need to be in the same partition for correct results. So as a rule, if you do any kind of aggregation, choose your key accordingly. Another popular scenario is cashing. Imagine that our dispatch service needs to retreat some customer data. There's really changes, for example, prefers car type. If these data rarely changes, the service will likely catch the value in memory with random partitioning. Each instance would have to catch valleys for all the users we've partitioning by user. I d. Each consumer will have different subset of this data saving memory. You should also thing if the key you choose will balance messages well, imagine that our booking message also contains service tear normal or premium. Using this information of her partitioning, it's not the best idea since given many more normal messages. The partitions would have different size, and one instance of the service would have to process many more messages. It is also essential that your kiss card in ality is same or larger than the number of partitions. Otherwise, you will not be able to spread low this needed with this steer information having to values, even if we created our topic with 100 partitions on Lee to off them would get any messages. Last but not least, think if you care about the order of messages Kafka guarantees at best or the ring within a single partition. If, for example, you want to be sure that evens for each customer are consumed in the same order as when they were sent, you must use customer ideas. The key things I mentioned are not comprehensive, but I hope that they give you understanding off what you should think about when choosing a key. Be aware that tough guys a very flexible tool and allows you to provide completely custom partitioning strategies for producers. This is out of scope for this course, but if you ever find yourself in a situation where you cannot find a way to choose a key for effective partitioning and or need to find tuned Kafka for optimized resource usage, this is the way to go.

Automatic Retries
[Autogenerated] Cathcart producers have also one useful feature, namely the automatic rich rice mechanism for recoverable errors. This means errors that are caused by failures that are transient by nature and likely to disappear promptly. One example of that is when the broker to which a protester is sending messages is unavailable for some time. For example, do you to network problems. This will be handled by the producer client automatically. The client will attempt to read deliver the messages unless it reaches the maximum retry slim it specified by the re tries configuration property, but no longer than specified by the delivery time out milliseconds. After going over one off this limits, an exception will be thrown by default. There is no limit on Reprise and the time out. It's set to two minutes. If you want to change this behavior, usually the way to go is through the delivery time out milliseconds. We will citizen action in the next demo off course. There are other errors that will not result in re tries. For example, when you provide the wrong through, realize er, trying over and over again will not change anything, so the exception will be drawn immediately.

Batching and Compression
[Autogenerated] in one of the earlier examples, after preparing a producer record Descend Method was invoked hold by Flash to make sure that the record was actually sent. What does this mean? Doesn't calling send guarantee that a message is sent? This sounds a bit weird, but in fact it is not. And to understand it, we should see what happens behind the scenes. When you call sand, the producer will serialize the key and value using this, realized her classes provided and past the record to the partition er that will choose the right partition. Then the message will replace the in a buffer where it will wait for a dedicated thread. This thread will pick it up and send to the Kafka broker. So there is a period of time after a cult of sand in which the message is still waiting to be sent. As I mentioned before, Kafka was designed to sustain enormous through boots. Place would not be possible if each message would be sent in a separate request. Therefore, when the spread responsible for sending messages is ready, it will take all the messages that are in the buffer and send them as one batch you can cover the maximum size of the but by specifying getting bite. Using the batch size property, you will usually want to decrease it. If you have limited memory on their producer and increase it to achieve higher throughput, know that this property sets a cap and the trade will send messages anyway when it is ready. If you don't care about late and see that much and want to increase batch size to be more efficient, you can use another property, linder milliseconds. The messages will then Onley be sent after the time specified by lingering really second passes or batch sizes attained. These basically lets you straight between late Insee and strip food, depending whether you want to deliver high volume of messages efficiently or if you want to deliver them as fast as possible. Let's think about the booking service when the quote except that event is produced, the user is waiting for the information about the car and estimated pick up time. Time is of essence here, since we want to serve our users as fast as possible. Of course, the millisecond lost wants change anything, but it's better not to wait 10 seconds before the dispersed service receives the message and can take any action, and the volume of data is relatively low, so we can probably let the producers and messages as soon as possible. On the other hand, imagine we hit another topic. Car locations where drivers devices would send their location ever. Second, please would be a very high volume topic, not so late in the critical. So I'd say forcing larger batches would be a good solution. No, that's sending about just some constant overhead. Therefore, especially for small messages, it makes sense too much money off them together. There is also one great benefit to botching the data that I really need to show you by default Kafka, since the bite s serialized by the serialize er's. But you can additionally enable compression by specifying one off the supported algorithms using the compression type property. If this is done, the producer will compress the message. And what's more, it will then compress the entire batch, giving you even better compression ratio words. Even more interesting messages received from the producer are stored on the broker and sent to consumers unchanged, therefore, being able to squeeze more information into same number of bytes will save your network bandwidth on both sides and the disc space on the broker. Compression, of course, comes at coast of the CPU time and a bit of extra late Insee. But you can just from different algorithms traded between the speed and compression ratio plus CAFTA producer. Usually it's not a CPU heavy process, so using compression is recommended.

Demo: Retries, Batching, and Compression
[Autogenerated] in these demo will see how re tries betting and compression work in practice. In the first exercise, I want to show you how benching and compression can impact flow off messages and storage efficiency. We will begin by starting her docker containers in the detached mode, bypassing miners the option. Verify that the containers are running in the background. You can use the Docker ps common. We will come back here in a second, but let just use the same code as in the previous exercise to create our topic with two partitions now back to the containers by running them in detached modes from the same terminal, we can connect to the shell inside the Kafka container by using Dockers. Exact comment. After entering the container, let's navigate to the place where calf casters the messages, which is by default of our Leap Capta Data directory. There you can find directories directly corresponding to our partitions, where all the messages are physically stored. So the size of these directories is the size that the messages take up on the broker. We will use this terminal session to check the size of the partitions between the runs off the producer. Let's go see a new piece of code that I prepared to send larger number of messages. It has a lube that produces 5000 messages in total and pauses 10 milliseconds between each giving the rate off 100 messages per second. These balls will help me illustrate how watching behaves. The first task in this exercise is to completely disable botching. This is achieved by setting the batch size zero and guarantees that each message will be delivered separately. Let's run this and wait as it should take almost a minute to complete one run. When it's done, we can go back to the Kavika broker and check the partition size. We have 472 kilobytes compared to 16 kilobytes before, which means that messages sends take 456 kilobytes. We can go back to the code and performed the next step of the experiment, which requires three enabling default but size cup and modifying settings so that each batch will contain around 10 messages. It takes a run 100 milliseconds to produce 10 messages, so we should set linger milliseconds to 100. This guarantees that the messages will be sent on Lee every 100 milliseconds, unless the batches to beak. But this will not happen. We can run it and verify decisive the partitions again. This time the increases off 196 kilobytes, which is much less than before. Our messages are small. Therefore, reducing bunch overhead is so visible here. I hope this makes clear that, but think it's usually a desired behavior to prove that it works. As I say, we can produce another round of messages this time putting around 100 messages in age botch. This means that we should wait 10 times more, which requires increasing Klinger timeto one second. After sending the messages, we can verify that the increase in partitions size was even smaller, which proves that benching can improve storage efficiency. The last to do comment in the code instructs to enable compression, we'll use the same bunch settings as any less run and provided G. CPAs the compression algorithm. This one will give us a particularly good compression ratio. 40 complete least of algorithms. Please refer to the capital documentation. This inevitably leads to the smallest increase in partitions size, this time only 52 kilobytes. I hope this has shed some light on Kafka producer benching settings. Last thing I want to show you in this demo is come automatically tries work in practice. I have preferred. If I with go to produce two messages we've half minute post between them. Let's run the producer and after it sends the first message, let's stop the broker. These can be necessary, for example, when you need to reboot the broker in order to apply a critical security update, we can execute such scenario easily. Using Docker compose stop command and specifying which service to stop. We have the broker down. Producer Kennel sent the second message, but you can see that it's still re tries to do it over and over again. If we boot the broker now using Docker compose start command. As soon as it is able to serve producer requests, the message will be successfully sent. I think it is a really cool feature of Kafka that such common problems are solved out of the box by the client implementations

Consumers
[Autogenerated] in the previous module, you learn how customers operate with him. Consumer groups. We discussed that each partition has its own dedicated consumer and that after changes in the group, everybody's happens. I also mentions that consumers, by default, commit officers behind the scenes. We will no take a closer look at all these things to better understand Kafka's behavior. Let's go from the start. One of the consumers in a consumer group will always be the 1st 1 to come online and contact the calf. Kirk Laster. In such case, ____ Lester chooses one of the brokers to become this group's coordinator. From now on, the group coordinator is responsible for keeping the least off active consumers within the group whenever it seems that there are any changes to the least, either because the new consumer shows up or one off the active customer sleeves, it will trigger a partition, rebalance during the partition, rebalance all partitions, assignments are revoked. The group coordinator then sends the least off all partitions and active consumers, the one off the consumers designated the group leader. The group leader is responsible for assigning partitions to consumers and returning this information to the broker the broker than informs individual consumers which partitions they should consume, ending the rebalance and letting the messages flow again. There are two things that I would like to highlight here. First, there is always some time without processing after the partitions air revoked. And second, when membership in the consumer group changes, some partitions will change ownership. It means that, for example, if you're kissing some perky information like we discussed before, you will need to refill. Your cash is therefore, partition. Re balance is not free, but this is usually acceptable as three balances know that come on scenario in a healthy application. Also, this happens when you think one of Kafka's building partition assignment strategies. They distribute them more or less equally between consumers, if needed. If this process can be entirely customized, for example, if you want to rigidly assigned partitions to consumers by hand. But this is rarely needed, and definitely it is an events topic that I will not cover in this course

Offsets
[Autogenerated] Let's now talk about the offsets. In the previous module, you saw a live example of a consumer starting from offsets committed before by another consumer. This was an extremely simplistic example, and now I want to take a closer look at the offsets mechanism to save you some surprises. When using calf by yourself, I will focus on explaining the out to commit feature that is enabled by default. Understanding it well, we'll let you use full power of the consumer client without resorting to Monaco off that management, which can be a bit tricky to get right and I will not cover it in this course and offset can only be committed automatically during the call to the Paul methods. So at most it will happen once per iteration of the poll loop. And the offset committed is the offset off the last message returned by the previous poll invocation. Let's look at an example. Poll returns messages 11 12 and 13. We do some processing and go back to the pole. At this point, the consumer will assume that all messages returns in the previous poll were processed and therefore commit the highest off that returned by the previous poll in this case 13. Now this comet does not need to heaven on every poll. The frequency of the automatic comets is defined by the other comet interval milliseconds property. So if the offset is committed now, the timer will be set to zero on every subsequent poll. Only if time elapsed is greater than the value of out of committee interval milliseconds. The consumer will commit the offset and reset the timer. An important thing to observed here is that calling Paul with automatic comets enabled means that you acknowledge that all the messages that were received so far were correctly processed. These maybe sometimes hard to guarantee. For instance, let's say that in the booking service would receive three messages with trip intense. First and third receive immediate responses from the routing service. But for some reason, the second is taking unusually long. In such case, do not block the application. You should save it summer for re tries or analysis and continue pulling Kafka. One common pattern is to send such message to a special Kafka topic. By the way, an interesting fact is that the officers are store simply in a topic which is called consumer offsets. This topic leverages the feature of Kafka called Compacted Topic, which keeps on Lee the latest value for a given key.

Consumer Failures
[Autogenerated] before the demo. I still want to explain another important aspect of consumer operation. Taylor's handling. In the ideal case, whenever some exception happens in your application and it loses ability to process messages, you should close the consumer explicitly. These will immediately inform the group coordinators that it should trigger a re balance, therefore minimising the downtime. Unfortunately, not every exception can be anticipated. And sometimes things that can bring the consumer down immediately like hard Lor falls happen. In such case, the coordinator should know that it needs to react. This is handled in Kafka, where the heartbeat mechanism, every now and then, usually every few seconds, the consumer will send the heartbeat to the coordinator, letting it know that it remains active. The frequency or the heartbeat is governed by the heartbeat interval milliseconds setting. This has to be set in accordance with another property session. Time out Milliseconds, which tells the group coordinator how long it most should it wait for a heartbeat before deeming the consumer dead. Obviously, heartbeat interval milliseconds must be lowered in session. Time out milliseconds as otherwise, the broker will always think that the consumer is down and keep it out of the group. This heartbeat is separated from the message processing qlogic and calls to the pole method. So it may also happen that your consumer, for some reason fails to make progress. But the heart beats are still being sent, and the processing for these customers partitions is effectively blocked for such scenarios. You can also configure the maximum allowed time between consecutive calls to Paul is the kind of secondary heartbeat I suggest we jump right into the demo to see how these and things discuss before work together.

Demo: Consuming Messages
[Autogenerated] In this demo, we will again consume from the court feedback topic. But now we'll focus on checking how offsets are managed and what happens when a consumer goes down having a fresh docker compose environment. With the topic created, we will set up to consumers in the dispatch service Consumer group. The 1st 1 practically the same as in the previous module, will consume all the messages from the topic. The only difference is that the keys are now in teachers. No strings. We concerted already, so it registers the consumer group with a broke her. For this module, you will also find a helper Java class. We've called to display consumer group offsets. As the consumer already registered the group, we can try it out. You can see that the capture broker is aware off this group's existence, and since no messages were ever produced to the stopping, the offsets are zero. Let's now go to the second consumer, which is a bit more special as it is designed to get stuck after consuming few messages. Note that I explicitly said Max poll interval milliseconds to 10 seconds. This means that if more than 10 seconds loves between subsequent calls to pull the consumer will be treated as failed by the group coordinator, and his partitions will be revoked. The task in this exercise is to write code that will actually guarantee that this happens after processing five messages. The idea is to block the consumer from reaching the pole method and committing the offsets. To do this, we need to count messages as we consume them after every message will increment the counter and other condition to sleep for one minute after consuming five of them. Here, it's simply asleep. But normally this can also happen if the message processing takes much longer. Imagine that in our right hailing service, this particular booking is for a multi seater vom. But at the moment that this part service has problems finding Kwan. Let's also other message to know that we have preached this coat started this consumer along the 1st 1 and verify that partitions are re balance between both of them. Now we can produce a few rounds of messages and wait until we enter to sleep section. No, The interesting part happens after 10 seconds from the last poll. The group coordinator considers the consumer to be unhealthy, revoke Asti partitions and assigns all of them to the other consumer. And since the failed consumer has not committed last off that it processed, the first consumer will re process some of the messages at first sight. It may see very wrong in the context of her application. That's processing. Your booking twice means that two cars will be dispatched. Probably not. Even if this service dispatches the car twice. Some other component will probably very quickly detect and fix this inconsistency. These possibility of processing messages modern ones it's usually referred to as at least once guarantee. It means that no message will ever be lost. But you might say it twice. Getting so called exactly once guarantees in distributed systems is extremely hurt and comes at significant performance Coast. Therefore, I strongly encourage you to embrace at least one's guarantees whenever possible and design your system accordingly, so they tolerate duplicates of the same messages. These module comes to an end here. Given that Kafka is a huge project, it is not possible to discuss all the details and configuration properties off producers and consumers in few clips. But I strongly believe that what you learned here. We'll let you reason about systems using Kafka, designed them by yourself and easily understand Kafka's documentation whenever needed.

Fault Tolerance and High Availability
Introduction
[Autogenerated] for sure are single. Kafka broker will not be able to continue running forever. Even if the hardware was 100% reliable. We still need to perform upgrades and maintenance starts on the broker. In that case, when the only broker is stopped, producers would not be able to send any messages. While it may happen that your application is fine with that Kafka, it's usually used for essential data flows. Whatever data is coming in, made purchases from your store right bookings, user clicks, credit card transactions or sensor data. Usually there is no possibility to buffer all the data on the edge and wait for the broker to come online in a platform like a ride hailing service, calf, cow or other messaging system is often the backbone off the whole infrastructure. And if it is down, the whole platform sayings. Therefore, as thesis one off the basis off software that need to be always up, Kafka almost always is deployed as a cluster of several brokers. Apart from this planned maintenance tasks, there are other incidents that do happen, like hardware failures, power outages and other catastrophic events. So running a single calf broker, it's usually not a good idea, unless you're okay with the possibility off message loss and downtime, meaning broker being can reachable for extended period of time. This is very rarely acceptable. Therefore, they need to deploy multiple brokers. In this model, we will focus on learning how you can make sure that your Kavika deployment is always ready to serve your users, and it's a reliable source of information. We will focus on ensuring that our system is fault tolerant, meaning it can continue to operate with unexpected failures. Highly available, meaning that our cluster is always able to serve producers and consumers. And consistent meaning here is that if we produced a message with the right settings, we have a guarantee that it will not be lost. As I said, Kafka was built with this concepts in mind from the very beginning. So with the knowledge from the previous module, this should not be a difficult transition off course. Multiple brokers working is a single cluster require some coordination. First of all, there must be a single place, which is the source of truth about the members of the class ter. Also, there is always one broker that takes the special role off plaster controller. Whatever happens, there can be no doubt which one is the controller? This place in the Kafka cluster, which is responsible for keeping that order, is zookeeper. Whenever a new Kafka broker joins the plaster, it will always connect to zookeeper in order to register itself and learn about other brokers in the cluster. This is pretty much everything that zookeeper dozen Kafka context in the production environments. The keeper always runs in an insensible, usually off free or five notes. In this model, we will use a single zookeeper, an estimate. It's reliable, as I want to focus on Kafka brokers. An important fact is that Kafka usage of zookeeper is very limited. Therefore, if you have a zookeeper and assemble in your organization that can be shared, you will most likely want to use it. For Kafka, there is no point in deploying. Can you one

Message Persistence
[Autogenerated] I think that, but starting point for our discussion is the physical location of Kafka messages in the previous module. In our single brokers set up, we visited directories in which the Partitions data was stored. So in principle, each message that is successfully sent and raising to the directory it's safe. But what does it mean from the producer's perspective? How can you know that the message was written to disk? First, you need to configure properly what constitutes a successful sent for you. And this is done by adjusting the producer a C case property That sense for acknowledgements, the most relaxed volley for this property is known. It means that the producer will consider a sense successful when it manages to send the data over the network, even if it never reaches the broker. I think it is quite evident that this setting does not give any delivery guarantees, and it's highly likely to cause days. The loss, as any issue between the producer and the broker, can lead to death. This may be acceptable for scenarios like user tracking data, but not in majority of cases. Therefore, the default value off the A C case property it's one. It means that after a batch of messages sent the broker right it to disk in the respective Partitions directory and returns an acknowledgment to the producer. In such case, the producer knows that the batch was persisted. The disc in deliveries considered successful. Do we precise in the producer coat? You know that the acknowledgement is received when they get for record. Meta data returns After writing. Today's broker considers the messages to be committed and makes them available to consumers. This seems to suggest that if you are OK with a broker being done for some time, let's say, for a system update, having just one broker is fine. As long as the disk is not malfunctioning, no messages should ever be lost. This is not entirely true, and to understand these, we need to look closer at ____. Messages are written to disk and often overlooked stopping when talking about Kafka's reliability. When data is written to disk, it is in reality, written to a buffer in memory. These buffer is periodically flushed to the physical disk and will be flushed on clean shutdown. But if our server loses power or system shuts down abruptly data may be lost. Therefore, Kafka's documentation strongly suggest relying on replication, which means multiple brokers for reliability. On top of that, it is not safe to assume that this will never fail. Therefore, I hope you see that multiple brokers are really needed to provide proper guarantees for message persistence.

Replication
[Autogenerated] equipped with this basic knowledge about message, persistence and knowing quiet. One broker is not enough. Let's see how we can build a more reliable system with work after brokers. For a high availability scenario, calf gannets at least three brokers while running a cluster with two brokers. It's possible it is not recommended for production deployments, and in few minutes you'll understand why. Later, we will also talk about when you might want to have more brokers. You may remember that in the previous modules, the coast to create the topic contains three arguments. I advise you not to care about the last one the replication factor and now it's time to actually talk about it. Replication factor specifies for a given topic How many copies or replicas should be maintained for it on different brokers? If we said the replication factor to three, it means that we will keep three copies of the partitions. They down the cluster. Note that you cannot have two replicas of the same partition on the single broker because it would kill the purpose of providing false tolerance. So we've replication factor of three. You needed Lee three brokers. Let's not race the message flow, just like we did a moment ago, but this time for a replicated topic with a replicated topic. Different brokers will keep a copy off this topic partitions data, but producers and consumers communicate exclusively with one broker, which is called partition leader. The producer sends messages to the leader, and the leader writes them to. These with the default value of a C case won it since the acknowledgment to the producer, and they send this consider successful regardless of the producer activity. Brokers that are not partitioned leaders but have replica of this partition assigned constantly issue requests to the leader in order to get the latest messages and write them to their desks. The leader keeps track of debt, and when it knows that the batch was written, toe this for each replica it considers the message is committed and makes them available to consumers. Now, despite having the safeguard in form of replication with a C case equal toe won, there is still a chance to lose messages, which I can explain. On a simple example, we sent some messages. They get persisted on the leader, but before the replicas fetch them, the leader goes down and you broker becomes the leader for the partition and the message is lost since the for these air things. It was persisted, but it's not true. That's why there is 1/3 Valley for a C case available. Oh, it means that only after all replicas right, the message and acknowledgement is sent to the producer. This seems to provide strong guarantees. But what happens if the connectivity between the brokers is interrupted and one or more replicas are not able to get new messages? This is where the concept, often in sync replica comes into play. If the replicates requesting messages from the leader with no or little lag is considered to be in sync. But if some time passes and the messages are not requested, the broker that is the partition leader will consider the replica to be out of sync. A referee cut its out off, saying it's not considered when deeming the message is committed. It means that if you produce messages with a C K sickle toe, all but one replica has fallen out of sync. The producer will receive acknowledgement when they are recent to replicas. Also, consumers can read them at this point, since they are considered committed. This is my seem a bit strange. We are keeping replicas for reliability, but here away for inconsistencies is open. If we had to replicas out of sync, their leader would happily acknowledge the messages on Lee after writing them to its disc Fear note. Kafka has another setting men in Sync replicas, which we can use to define the number off minimum A replicas that must be in sync. This is the finds 40 whole classier and can be overridden on the per topic basis. Then, if the number of instinct replicas falls below the target, the leader will refuse to write messages from producers in case of replication. Factor of three. If you care about not losing messages, it's good to set this to two instead of the default value of one piece, safeguards against a scenario in which you have one broker off another loses network connectivity to the partition. Leader and leader still accepts messages in this unlikely scenario off. One instant replica and minimum set to producers get an exception, informing that there are not enough replicas up. That's why if you care about your data and availability, it's best to have at least three brokers and replication factor of three on each topic, mixed with instinct replicas set to it puts you on the safe side and gives the flexibility to perform maintenance tests on brokers without any delay. Let's jump into a demo that will show you how to run a multi broker cluster and illustrate the concepts that I have just introduced.

Demo: Multiple Brokers
[Autogenerated] this time we need to change the docker compose file to include the street brokers in our deployment. Instead of single Kafka, I will define three services. Kafka one, Kafka to and Kafka. Three. We can copy configuration from the existing Quan, but it requires adjustment to the advertised listeners. Variable. First of all, we need to match the name off the service in the entry provided for other Kafka brokers. So in case of the first services will break of Kirwan and without surprised Kafka to and Kafka three for other brokers. Now we also need to adjust the ports that which cuff Chris accessible from outside the Doctor Network on local host. We need three different boards for these. So I suggest putting 1991 in 1993 a long 1992 for simplicity know that we are not giving any explicit information to each of the brokers about the other brokers. They will discover each other for a zookeeper, and that's one of the reasons why you cannot have a Catholic. A cluster refers to keep her. We can now start a whole cluster using the same joker composed commanders before now you should be able to see four containers, one zookeeper and three CAF Cup brokers. This means that now we can create a topic with replication factor of three. Note that when creating the client, I'm not providing addresses off all three brokers. If I provided just one or two, as long as these brokers were up, everything would be fine. But by providing coal addresses, you get a guarantee that you will always reach the cluster, even if some of the brokers air down. Apart from setting this replication factor, I would also like to explicitly set minimum number off instinct replicas to to This can be done when constructing the new topic. Object by. Additionally calling conflicts method and passing came up as the argument we will keep on using two partitions for simplicity. Let's go to the council and create the topic after it is created. Let's check if the number off instinct replicas is indeed said to We can do this by checking Kafka logs with the use of docker compose looks, government and filtering the output. I am using select string because I'm running the sun windows. If you are on Linux, Sir Mark, you will want to replace Selleck string with drab in the output. We see that our properties said to to just like we wanted now it would be helpful to be able to see which broker is the leader of which partition and confirmed that each partition really has street replicas. This can be easily achieved using the admin a client. Here we use a method of declines described topics which takes a list of topic names. In our case of Lee one and the Returns topic names with their respective topic description objects. Then we have a method to print some details for each such name and description pair. For its description, we can get a collection of partitions and for each of them, we print its leader. And all replicas, let's see, was the result of running. It is for our cluster Percival. We have two partitions with three replicas each, and each partition is further for us old brokers. Each partition has also a leader on different broker s. Kafka will do its best to distribute the load among the brokers as evenly as possible. The ideas you see here are ideas assigned automatically to the brokers. There is also a mysterious rack property that this new inal cases. What is that? So it's broker has a unique I D, which is the basis for replicas distribution. But apart from that, we can assign it Broker to Iraq. Bracket is simply a set of servers that are physically CA located, therefore, have increased chance of failing all at once. If you provide rack ideas, Kafka will distribute replicas across all racks in order to provide the best possible false tolerance. The wreck name for the property doesn't mean that it has to be a physical rack. You could as well map it to club provider concepts like availability zones. We will not simulate racks in our demos, but you will see later in the course how you can configure them. Let's no focus on verifying give. Our cluster is fault, tolerant and highly available, as we expect it to be. For that, we need a producer, which can be the same as the basic one we used in previous modules. One thing I will do is the change acknowledgements valley toe all in order to get better consistency guarantees. Let's produce one round of messages to check that everything is running fine. Now it's time to inject our first fault by bringing one off the brokers down using docker compose. Stop comment. If we check the replicas information. Now you will see that each partitions has only two replicas. And if we happened to stop one off the leaders, another broker became the leader. We can produce more messages without issues, since the messages are persistent onto brokers, and that's enough to satisfy our constraints. Therefore, our system is Kylie available since problems with one broker to not stop clients from using the cluster. But to make sure that we are guarded against bigger problems and we don't lose messages, let's stop another broker. This results in both morticians being Clift, with only one replica and complicated on a single broker. If we try to send messages now, we will receive errors caused by lack off preset number off replicas. No, that that the producer is retrying. Since this is a kind of error that is expected to disappear sooner or later, we can bring one off the brokers back online to see that as soon as the partitions have two in sync, Ripley costs the messages are successfully produced noted that to obtain these behavior, it is essential to set acknowledgements toe all. If we said this, the one even with the men in sync replicas set to messages will be produced successfully, with only one broker running, which is not very safe. One interesting thing here is that despite the possibility off changing the leader for a partition in case of problems, Kafka will always try to come back to the original state by returning the leadership to the original leaders. It may take some time, but will eventually happen in orderto keep the load balance between the brokers. I hope that this example has given me a better feel of how a multi broker Kafka cluster works. Health problems in such deployment are handled and how you contend different settings to get guarantees that you need.

Data Retention and Cluster Sizing
[Autogenerated] earlier in the course, I mentioned one of the great features of cuff got. It lets you build reliable applications. It's flexible data retention mechanism. One example I gave was the full of wink. You deploy a new version of a consumer application only to find out after a few hours that it has a serious bag and you need to redeployed and process the same events again. It is one off the moments where some longer term data retention is priceless. The question Always these. How much data should be stored and for how long. This will vary from use case to use case, but I want to quickly show you how Kafka keeps the data on disk and how you can tune its retention policies according to your needs. When we visited Kafka's data directory previously, we saw two partitions having their director is there. Each of them forms a Kafka log corresponding to a single partition replica, so old messages belonging to a given partition are appended to files Inside. This directory notes that this directory must be a single mount point, so it's either one physical disk or managed array off this, like right, you cannot distribute single partitions data across many regular days. So this is another thing you should consider when choosing number off partitions for your topics. Inside age directory calf costars individual files called look segments messages are edit to a segment until it reaches a pretty find size or configured time elapses. At that point, that segment is closed and then you on this open. There's always one active segment, and all incoming messages are appended to it. A segment can be close to either when its size exceeds lok segment bite size or it has been open for Lok segment milliseconds, both being properties of a broker, which you will learn to set later in the course, knowing that you can define your data retention policies for the whole look directory corresponding to a single partition. You can do that by setting cloak retention bites and local attention milliseconds. These properties specify maximum space that the look for the whole partitions should occupy and maximum time for which the segments should be kept after closing. But it's very important is that only segments that are close can be removed. For example, if you want the data to be removed daily by setting cloak retention milliseconds, but it takes two days for a segment to feel and be closed. The data will be cleaned up only every two days, so watch out for days. The beautiful segment sizes one gigabyte. So unless your topic is a very low production rate, this should not be an issue that the force for the Spurlock settings are one gigabyte and one week they make sense. But for example, if you have a high volume topic, this one gigabyte will be reached much sooner. Therefore, you should increase local attention bites. In any case, you should always gather your requirements for data retention and then do the calculations. Speaking of calculations, let's also talk briefly about cluster sizing before finishing this module for most topics. If you care about not losing messages, you will want to run three replicas. This gives you at least three brokers. The more topics with more partitions to create and the more producers and consumers connect to your brokers, her bigger will be the load on the individual brokers in a real life person of the right hailing Cup, three brokers would most likely not be able to handle all the load first, that the space or throughput could be not enough to serve all the partitions with the desired retention and performance. And second, the network interfaces may not be able to keep up with the volume of incoming and outgoing data. If this is the case, you will have to deploy more brokers in the class. Ter. Luckily again, Kafka was built to handle scaling without problems. So generally you just need to add brokers and let CAFTA handle the distribution of partitions and their leadership across the brokers. To not be surprised by Kafka's under performance or do not spend too much money on the huge plaster. Without a necessity, it is essential to do your calculations up front. You should simply take a rough estimate of how much they that will be coming in and how much will be going out for its broker. Then see if the days can network interfaces will handle it. Exact resource optimization and sizing is out of scope for discourse, but with such pen and paper exercise analyzing the data volumes, you can get reasonable values that will help you size the class. Ter. Let's not recap what you have learned in this module. First, you saw that a single Kafka broker is nothing off for most of the scenarios, and therefore, a cluster of at least three brokers is needed. Next, you learned how Kafka can provide false tolerance, high availability and consistency in a multi broker deployment, and how configuration off your producer is also important to achieve these. Then you so in practice how you can configure multiple brokers in a docker compose deployment and verified that capture can provide mentioned guarantees. Last but not least, I very briefly introduced the basics of data retention and closer sizing. I can imagine that this module may have introduced a lot of new concepts, so take your time to let the things sink in before jumping into the next one.

Serialization
Introduction
[Autogenerated] in this module, we will talk about serialization formats in Kafka, which means focusing on how exactly the messages we send our serialized two bites and then this, he realized from bites on the consumer side. So far in the course, the data we've been sending for Kafka was in the form of strings and in teachers, while having playing in teachers as kisses. Realistic, sending the values explained strings. It's not in the real life. You usually want to put more information in a single message, and the string is not well suited for that, especially in a loosely couple system like Kafka were. Very often, consumers and producers do not share the code base or are even written in different languages. Consumers receive row bites, and they need to have all the knowledge needed to extract unnecessary information. So there is a need for well defined structure in this data. If we want to represent some data in a structured way in our producer coat, we usually end up creating a special type. Let's for a change. Take as an example a trip intent message. We've custom type. The strength serialize er will obviously not work. Therefore, we needs to provide some other way to convert our object to bite that will be sent over the network, sending structure data over the network in the language agnostic format. It's not a new problem. Therefore, there exist several four months that are used for this purpose. One of the most prominent ones is Jason. You can find Jason based AP Ice all over the Internet. It is understood by lots of people, and as it is, states based, it is human readable. Also, almost every programming language has at least one library for serializing condition. Realizing Jason so it seems like a good candidate. And in fact, there is nothing stopping us from using. Jason is the data serialization. Formatting are cuffed applications. There are, however, reasons to not use it. Percival Kafka needs to store the data that we sent to it. Same as with compression. The less bys we used for our messages the better. And Jason is very verbose as it is. Simply a textual representation of your data are many mile trip intent. Message in Jason could look like that. It contains user right e and latitude longitude coordinates for the pick up and drop off points. Of course, this is not enough for real application, but let's stick to this now. Having this Jason means that after serialization, each message will contain all the data, including the keys, which is a waste of network bandwidth and storage. With one message. This is not so bad, but with 1,000,000 messages per day, something more compact would be better. Another problem with Jason is that it is not great in fast changing environments. Changes to the structure usually require manual changes to producers and consumers called. And while public facing Web AP eyes that use Jason are usually carefully designed and kept a stable. It's possible company's internal data structures may evolve much faster and in more chaotic way. Some fields are defecated. New answer. Add it and the names are changed. Imagine that for some reason, we need to move from little on strings toe individual floating point numbers. Jason. It's not the best format out there to accommodate such dynamics. Jason has also non negligible serialization and de serialization cost and well, this is not the primary concern. It would be nice if we could improve on that. So we're looking for some data exchange format that would be compact first language agnostic and could easily accommodate evolution of the scheme. US N 30 World Off Serialization Frameworks. The idea of a serialization framework is to offer some kind of a programming language independent format for describing serializing and d serializing structure data. The court difference between such serialization frameworks and text four months, like Jason or XML is that they are much more compact, faster to serialize, have well defined rules for ski my evolution and come with tools that automatically generate code for different languages. One of the reasons for their efficiency is that they are not text based, so the binary representation off serialized objects is mostly not human readable. There are several different frameworks out there, but I will focus on one off them protocal buffers developed by Google. Other framework quietly used with Kafka and the one that was championed by con fluent from early days, is Apache Afro. They differ slightly as ever, was started in the big data world is a former for data while proud above its more targeted towards inter service communication. But both are used in different ways in different organizations, so it's good to be aware of the existence off both. And after going through the probe of examples, you should have no problems working with Avro if needed. When using this frameworks, we always need to define the schema for our objects up front. Pratibha for a treat Intent message from previous slides Looks like that. I imagine you may be seeing this for the first time in your life, but I think this format is quite clear. We have types and names of the members, plus orginal numbers to have a clear ordering within the records. Such definitions are put in files with proto extension having the definition. Not we need to somehow use it in our application. That's where the coast generation part of the framework comes in. Using the problem of compiler code proto See, you can compile this photo file into different languages, and in our case it will mean Java classes to nicely automated there plug ins for different built systems. I will show you how it runs in the demo, but what's important is that the effect of compiling this proto is a Java class named trip intent, which we can use to create new objects of this type and get their properties. I think this is really cool. That's just by defining this schema, we get the coat for different languages for free. Now is a very good moment to move on to the demo, to see all the details of this approach and how you can combine proto bath with Kafka.

Demo: Protocol Buffers
[Autogenerated] since we will not talk about replication in this demo, let's go back to the development set up with one broker. As I've explained a moment ago, if we want to use proto buff, the first thing is to create the message definitions. Which language has different conventions were to put such files in Java Director recalled Proto. Next to our Jeffer Files folder is the standard choice. If you work on a larger project brought above, definitions are usually put in a separate package or repository in the file. Apart from the message definition, we also have the proto package and some Java specific options. Now the question is how to generate Java code based on days. In this course, I am using Great Deal build system and is a quiet, popular one. It has a plug in that will do this task for us. If you are familiar with grade. Oh, you can take a look at the built file for this module to see how this plug in is enabled and configured. If you usually work with other build system or other programming language, you will have to search for the canonical way to run these Luckily you brought up off its very popular. So you should have no problem with that. In the case of Great Ally, the plug in other built task called Generate Proto that we can run from the command line and it will generate the necessary classes. Having that we can move on to the producer implementation. First thing I want you to try is to construct the trip intent object. This can be easily done. By important. The closet was generated and populating three paint and fields. Now we need to serialize this message two bites and sand. In this case, we will convert the object Tobias before passing it to the consumer. So for the value type, we will just bite the race. We then need to adjust the serialize er to use Kafka's built in bite, a racy realize er. Last thing needed is the actual conversions from our Object to Abita. Ray Protoblood generates a method called to buy to Ray for days, so it is simply a matter of calling it. Now let's see the consumer, the serial Isar must always match the serialized were used on the producer, so we need to set bite array. The serialize er here also for the consumer itself and consumer records we need to use by to race diaper meters. Now, how do we get the trip intent? Object first After calling volume method on the record, we will get the bi tary. To make the conversion, we need to resort to a generated method parts from and possibly by to raise the argument. Now we can access our records members in a type safe way. So well, let's up the coast to bring the locations and check if this works together with the producer. No surprises here. Everything worked as expected.

Schema Registry
[Autogenerated] we have successfully employed prote above to represent messages in our system. And even if you were not able to appreciate speed and compact size of the messages, you so how convenient it is to use automatic code generation and have a type safe way to manipulate these objects for free. We could stop here. But there is one Kafka ecosystem component dedicated to handling message for months that is often deployed along the Governor Cluster Schemer registry. The role of the skimmer registry is to provide a centralized place where anyone and, most importantly, producers and consumers can register and query message scheme us Exactly. It's cases can be different, but the most crucial one is the following producer. When sending the message, will register the schema in the registry. The long Quidditch message. It will pass the reference to the schema, and any consumer can then retrieve it when reading the message. This is especially useful when you have a consular that is discovering the message skim out when reading it. In such case, you don't need to have a fresh version off the proto generated code on your consumer, but you can discover the schema after receiving the message schema registries especially useful in large organizations with lots off services and different message types. Let's see how we can use it in practice.

Demo: Dynamic Schemas and Schema Registry
[Autogenerated] to use this came a registry. We must include it in our deployment. The one most often used with Kafka is this schema registry from Con Fluent. So let's use it. It has a docker image available similarly to CAF can zookeeper. We need to provide the host name variable, which is required but only relevant if you run multiple knows of the schema registry and others also keep. For instance, also, we need to expose pours on local host for our producers and consumers. And it will report 80 81 since that's where the registry listens by default. After starting this deployment, you should have three containers running. The producer coat will construct the object the same way as before using the generated code. But we should make some changes to the way how it is serialized in sent first registered a trip intern scheme automatically, we need to add a schema registry u R L property so that the producer knows where it can do that. Also, this not requires a special serialize er from con fluent Kafka brought above to realize er, with these two changes, we get additional benefit off, not having to convert the object. Two bites Mannelly. Therefore, we should change the bite. A race to the trip intent objects and removed a call to the serialization method as it will be handled automatically by the serial Isar. Let's now see the consumer, as in the producer, we need to provide the registry your l and change the d serialize er to the proto buff one. Now what will be the type of the records value we could with other problem? They serialize it to the trip intent object. But as I've said, I'd like to show you another interesting approach, using the customer without exact knowledge of the message. Four month. If you're consumer has no access to the generated last definition, it is possible to the serialized bites interpret above type dynamic message, which can hold any protoblood message along with the scheme. I information this may, for example, be useful when you have a consumer that needs to read messages from different topics, and those messages contained partly the same type of information but do not share a common structure. Consider hypothetical example. We're apart from the trip intent. You have a message pickup drive that for some driver has its current point as little on From and the pick up point as lat long to, and you want to fetch these messages along the strip intent once in order to create new ones with three points. This may seem like about design, but the truth is that in a large organization you will often have to work with imperfectly designed systems, and dynamic messages can help with that. Let's now is an exercise. Write the code to look for a latte long from field in the message and print its value. One way to do that is to go over all the fields in the message by calling Get all fields method on a dynamic message. This returns and mapping from what is called feel descriptor toe field values. So for each field, we can check what its name using get name method, and if it is what we are looking for, we can print it using the value directly. Let's run it, produced one message and verify that it indeed works as expected. Know that you can also use dynamic messages on the producer. If the generated code approach becomes too rigid, just be aware that there is always some performance penalty associated with dynamic messages

Summary
[Autogenerated] in this module, you have seen how you can take advantage of buying a re serialization four months in order to improve the efficiency and simplify schema management. I hope you see that those can give you a real advantage over plane tix or custom four months and can definitely help to keep order and protect from some failures caused by former mismatches. Be aware that we have just scratched the surface and details off the schema. Management like schema, evolution rules and the details off operation of a schemer registry are worth their own course. On a side note, I used critical buffers for examples here, but as I mentioned before, capture from early stages was very average oriented. If you need to use Avro, the way it works will be very similar from schema definition, toe cogeneration and dynamic message handling. Please just note that while probable former can be used without schema registry and I showed it in the first demo, this is not the case of Avro. The reason for that is that ever by design, requires the schema to be present in every message and ascending it with each message in Kafka, with result in significant overhead. The registries need it in order for producer Toby able to send just the reference instead of the complete schema, whether you use skim a registry or not, or what format you use will be most likely a decision taken after long discussions and tailored to the specifics of your organization. But even if the registry is not used everywhere, understanding the concepts from this module is essential for any engineer using or managing Kafka.

Installing Kafka Manually
Introduction and Prerequisites
[Autogenerated] in this module, we will work our way through the task of going from the bare bones Debian Linux looks to deploying from multi broker Kafka Plaster services are deployed to production environments in a multitude of ways, and the one you have to use depends on what issues that your organization to be as generous as possible and not introduce unnecessary cognitive load. I will continue using docker compose. We will just stop using revealed images from con, fluent and built our own from scratch. This time we will run three zookeeper instances, as we would do in a production environment and three Kafka brokers. This may be surprising, but the number of steps to get all of them running is not so weak. Also, I hope this module will not be too complicated. As the docker file four months. I will use this very irritable and easy to translate to other approaches. This module is almost 100% practical. So from now on, expect to see only the editor and the terminal in this module. We also have a darker, composed file, but now it is only filled with service names. We have three zookeepers and street of car brokers. We will have to build two container images, one for zookeeper and one for Kafka. And for this purpose, we have two directories. Each will contain everything needed to build the image for the respective service. The location of the built files is something that we need to indicate the Docker compose to do this instead of putting the image property for each service, like with it before we need to add the build property pointing to the directory for the image. So we need to pass Zookeeper tool three zookeepers and calf cattle three. Caf CASS Docker Images are built based on the definition placing a docker file. So we need to have docker files in both directories. There will be some duplication between them, but the idea off this module is to give you clear understanding of what is needed to run caf can zookeeper. So it is not necessarily a bad thing. The first instruction of a docker file from defines what is the base image for both? We'll start with a clean Debbie on distribution of flame nukes. The latest Doug means that we want to use the latest stable version. This image is available on the Docker hub and will be pulled from their on Differs build. Now when are containers start? We want them to run zookeeper or Kafka. The exact action that should be executed is defined by the CME. The instruction, which will always be the last one in both of our files. We will put the right comments later on. But for now, let's echo Hello, Kafka and Hello, zookeeper. At this point we have first and last instruction in place. So it is a good time to see if we can run all six services with Docker compose. First, we should build the images based on our definitions and that this is done using docker compose build. Having them. We can run docker, compose up and see that the hello Commons gets executed The container's exit immediately because that's exactly what the comment we provided us so looks good so we can start adding some steps that will take us closer to running. Kafka and Zookeeper both have the gentle development kid as a requirement, So this is something we need to take care off. One convenient way to get the Java platform in Sturgis, Asil Open J. D K Bills on the Oslo's website. We can look for the latest stable build for deep finally nooks. We need to download the depth package, but we need to do this inside the container. In that case, let's copy the link to the package and go back to the doctor files. We have the link, and now we need to get the file using some common line program. Percival to execute commands when building the docker containers used the run Instruction, a popular program to issue http requests only Nukes is Carl, and it will serve our needs. The L flak tells it to follow redirects, which are very likely to occur and oh, specifies the don't load path. We could stride running Docker bills now, but the current program is most likely not included in the base image, so we need to install it using the package manager on Debian. The package manager is a PT, and before we install anything, we should run update In order to synchronize package indexes, the package manager may sometimes ask you for some keyboard input, but we cannot respond to any questions during the docker built. So to make sure it does not wait for an input. We should, said the Debian front and environment variable to non interactive. After the update, we can install Curl passing their Y option to answer yes to anything and copy disco to the other Docker file is this part is the same for both images. Let's build it and very fighting Gervais. Don't load it. Note that thanks to Docker smart catching in subsequent bills, this download will not be repeated. Now it's time to insulted unloaded package using Comore low level Debian package tool that is capable off installing files in the format we have. This will complain about some missing dependencies, so expect some errors in red in the build output. To fix this, we need to follow this command with AP to get installed with F flag to first installation off those missing dependencies. If this succeeds, we can remove the downloaded file. Let's copy the coat and run the build again. Next step is to download Afghans to keep her. Both are projects of the Apache Software Foundation, so if you follow the links from the projects websites, you will end up on the generate Apache download site. The link for downloads are chosen based on your location, so we should not copy the link directly but rather let Apache servers choose the closest mirror when they build the images. Since in a real environment, the images may be built on build servers that are in a different place than you. There is a way to do that by calling the right end point on the Apache servers, which will return adjacent containing the preferred mirror to get the U. R L. We need to actually extract a single feels from this Jason, and it can be easily done with the Jake You program. We can assign the outputs to variable and use it immediately. In the Carl comment, the calf can zookeeper. Specific parts can be copied from the girls that are found on the websites. Note that Jake you it's not installed by default, so we need to other build step to insulate. Both products come as the tar GZ archives, so we should unpack them. I chose to put both under the uppity directory and remove diversion specific parts from the names, since it will be more readable in the rest of the demo. Let's make sure that the images build without problems and move on to the service specific parts. Since there are not many common parts. From now on, I suggest we focus first on zookeeper, since it is a prerequisite and then take care off Kafka.

ZooKeeper
[Autogenerated] Our task now is to basically configure everything properly and start the services. There will be two steps off the configuration one static that we do it container built time and other. This happens at runtime, bypassing environment variables to the containers I suggested to accommodate. For this dynamic part, we create a startup script that will be invoked in the CME. The instruction Let's start by moving kill zookeeper to a scream that will leave in the build directory. Let's go. It's start up, said the interpreter, to bash and place our ICO comment. Now how do we use this in the docker file? We need to resort to the copy instruction that will literally copied this file to our container. We can also rename it to make sure it's not ambiguous. Then we can simply use this script in the CME. The instruction. An important note here. The file will be executed inside the container, so it has to have UNIX line endings. I am creating this on Windows and simply coping it and trying to run will cause errors because of the difference in line endings between the operating systems to make sure that we are guarded against such _____ problems, I suggest installing a program called those two UNIX and running it on the file. After copping to make sure that the endings are correct, let's build the images and verify that it works. No, that's starting. A single zookeeper is enough to test it. Let's move on to create the Zookeeper Conflict File, which is by convention named Zue C of G. These it's a very simple file. The configuration starts by calibrating the Basic Time Unit in Zookeeper Take, which is set here to the seconds. We also need to specify the data directory for Zookeeper Storage Client Port, at which clients in our case Kafka brokers will be connecting to zookeeper and some timeouts for Internet communication. What they mean exactly, It's not super relevant now and you can check their exact purpose in zookeeper documentation. Now we have to add interest for the servers in zookeeper is several gives the same configuration and all servers are specified by adding I DS. In our case, it will be server one server to and server three. Now we have to indicate the host names for all of them and pours at which they can communicate with each other. We'll put zookeeper one zookeeper to and zookeeper three and use default port values. Note that each node will have the same conflict file. With this complete least, this means that server needs to be able to identify itself. For example, are zookeeper. One needs to know somehow that it is the one with I d one. Therefore, it should communicate with the remaining Wants these these candles for a file called My I D. That needs to be placed in the data directory in our case, configures too far. Sue Keeper. This is something that needs to be handled when starting the container. Therefore, let's add this to our start up scraped. We will take the server ideas the command line argument and corrected the mentioned file. Now let's add the missing base in the docker file. We need to create the data directory virus a keeper, copy that conflict file to the default conflict location and make sure that the line endings are in the right format in the sea and the instruction the common line argument with the idea is missing. So let's added by using server I d environment variable that we will set in the Docker compose file. Let's think is to install pro PS programa basically nukes utility that is required by zookeeper startup scripts Are we done? Almost. But you might have noticed that we still don't have any codes to start zookeeper. So let's replace the ICO line with the common to start the service, which is ZK server starts for round. Also, in order to have the I d set properly, we should add the variables values in the Docker compose file. Let's run the build and try to start the zookeeper servers. You don't need to read all the logs, but there are no terminal errors, and in the end you can see that all three notes are alive. So we have zookeeper ready. It was not that difficult, was it?

Kafka
[Autogenerated] Now it's time to handle Kafka configuration and installation. Let's also create the start of file like in the Zookeeper case and write the start coming right away. Tough guy started by running Kafka Server Start Command and passing the location of the conflict file. Let's make it a variable and studied at the beginning toe come server properties. I am doing this because we will have to apply some runtime changes to this file later on. But first things first. Let's feel this conflict file with things that are study First property without which Kafka will not run is the keeper connect. Remember the zookeeper connect variable We were setting in the Docker compose file in the previous models in the images from Con Fluent. It was then inserted into the server properties file. Then we need to set a listener security protocol map, which also corresponds to the environment variable we used in past modules. This time, let's use some more indicative names for the listeners toe explicitly state which one is for other brokers and which one is for clients. We also need to set the inter broker releasing their name, lest beat is too, said the look dear, the place where cup casters the messages to some persistent directory like var Kafka data. This is very important, since by default date I start in the TMP directory, which is not safe at all. That's all for the static part. Let's add instructions to copy, start up and conflict files to the container _____ line endings and make sure that the data directory exists. You may have observed that we have not, said the advertised listeners anywhere, and that's what we have to do on our container. Start up. I suggest that for simplicity, we handle all the dynamic parts by passing a broker i d. Variable to the container and effectively to the start off screen. Let's set it for all three services. You might see that it has a pattern. Kafka one, His i D. One etcetera. Let's leverage it also for pores. So for the broker with the I d one, the client port will have Suffolk Swan etcetera. In that way, we will be able to configure everything using only this broker I d. Variable. That's over. The docker compose, and we can proceed to the final bit, which is extending the configuration file dynamically in the startups creep. They're free properties that we must set in order for CAFTA to start. These are broker I D. Advertised listeners and listeners first broker I. D. Self explanatory and we simply to set it to the received argument value advertised listeners we have already used and discussed earlier in the course. I know that this time I decided to change the listener names, so we have to remember about that by using the broker i d. In the way we do now, we can use it to define above the coast name for the inter broker listener and deport for the clients. Listener listeners is basically the same as advertised listeners, but for internal broker purposes. And we can get its value by removing host names from the advertised listeners value. If we had multiple network interfaces, for example, one for inter broker communication and one for clients, we would put them there, and that's basically eat. Note that you can use this server properties file for any broker configuration, especially for things we discussed earlier in the course. For example, we can set compression type of snappy look retention milliseconds look, retention bites etcetera. The number of value properties. It's really huge. It goes into hundreds, and Kafka's documentation is your friend here. We should be able to run the complete cluster. Now let's try to bring it up. Looks like it starts without problems to double check. Let's create the topic and send some messages. Everything seems to work, and it means that we are done. Congratulations. You now know how to create a cover cluster from scratch. I hope that after learning how to install Zookeeper and Kafka all by yourself, your confidence levels have raised and you are ready to proceed to the final module in which I will show you some useful extensions for the Kafka ecosystem.

Ecosystem Extensions
Introduction
[Autogenerated] in this module I want to introduce to interesting additions to the Kafka ecosystem. So far, we have learned how to include zookeeper Kafka and skimmer registry in our deployments and such set up can already self lots of operational problems and give your organization a great platform. There are, however, two things that you might want to address, especially is your Kafka cluster and its user base grow. First one is related to legacy systems or less popular technologies. I mentioned earlier in the course that Kafka uses a custom protocol for all communications. This means that you need a client that understands this protocol in order to interact with Kafka. You may, however, encounter situation when you want to use Kafka with some technology that does not have a Kafka client or the communion devil of clients are not very stable, so you'd prefer to avoid them. Imagine that you have some applications written in PHP or PERL that need to be integrated with Kafka. There are some open source CAFTA claims available for these languages, but they're not a stable and well tested as the officially supported ones. So if you need something rock solid, relying on those libraries may not be the best choice. In such case, you may decide to use confluence tress proxy that provides http interface to a Kafka plaster. The other thing that you will have the sooner or later tackle is that when Kafka becomes one off the core parts of your infrastructure, it becomes essential to have it closely monitored and have easy ways to manage the cluster components. Some teams decides to build their own solutions around open source stooling, but some prefer to go for paid offerings. Which approach the choose will depend on many factors over perfectly valid, and it is up to the teams to decide what is best for them. In this part of the course. I want to show you how you can use confluence control center for that purpose. Note that the control center is a paid product, though you can try it out with a single broker for unlimited time and for 30 days with multiple brokers. So I have to tools that I want to show you the rest Proxy and the control center. Let's get to the _____ right away

Demo: REST Proxy and Control Center
[Autogenerated] in this module. We will go back to the confluence pre build images to have a first start with the rest Proxy and Confluence Center. I will use three Kafka brokers to have a realistic set up when playing with the control Centre. The scheme I registries also here, as I want to show you how it can help us when using the rest proxy. Also, we will deploy our two new components at once. They both have images available on docker hop. Note that as the control center is an enterprise off a rink, we should use the enterprise version of Kafka images that is preset with metrical reporters that will communicate with the control centre. We also need to explicitly configure the implementation off the metrics reporter and positive least off old Kafka brokers of a cluster to which the metrics will be sent. In our case, we will use the same cluster, but with larger deployments in multiple clusters, you might want to have a dedicated cluster for metrics. To deploy the rest proxy, we need to provide the image name in some mandatory conflict properties. We need to point it to the graphical brokers schema registry and indicate its external host name, which in our case is the same as the service name the port. It uses this 80 82 so we need to expose it on local host for the control center. We also need to just the right image, pointy service to Kafka and indicate where it can connect to the schema registry. Additionally, we said the replication toe one, since it's free by default, but for the _____ we don't need it replicated. The port used to access the user interface is 90 21. Let's bring it all up with a regular docker compose up. It will take noticeably longer now for all services to start. As we added this to services, So give it some time, even after a few minutes. When everything's ready, let's create a topic. First, note that at the moment, the http AP, I does not support stop it creation, so we'll use the code from the previous models. To do that. I am using version two of the wrist, a P I, which is the latest stable one. At the time of this recording, the topic creation functionality will be available in diversions. Three off the A p I, which is currently in preview. But after creating this topic, we cannot write to get information about it by issuing http request to not complicate things, I will use Java to issue http request. But remember, the Java has the most mature Kafka client, so you'll rather not use the rest. Brooks it from Java. But http request is the http request and he will surely be able to issue them with any language. To send the request we need to construct http client and the request object to get the info about the topic, we need to send a get request to this end point. We need to then call the send method which will issue their request and return the result as adjacent strings with the topic, configuration, details and information about port ations. This response is passed through a method that is responsible for four month in the received Jason nicely and printing it to the console. You probably remember that we have already queried this information before using the admin client. So now you see how we can get it using Http, If you run it, you will see the complete topic configuration and current partitions set up. We can now also try to find this information in the control center. To access the control center, you should open local host 1921 in the browser. When the start page loads, you can see all the connected clusters. In our case on Lee Wan, there is some general information here, and we can go for more by clicking on the cluster icon. Note that the information about expiring licenses presented on this page the first two tiles present US information about the brokers, including very useful production and consumption rates. Metrics. The current numbers are reflecting the traffic on topics that are used by the control center below. We have a topic tile showing 40 topics, which again is due to the presence of the control center topics. We can immediately see the little numbers off partitions and two zeros, which is good news as they reflect the number off out of sync replicas. You can also see things like connect or case equal DB, but they are out of scope of the score, so let's ignore them. First up, available on the left is the brokers one. It contains overview of all the brokers in the cluster, where we can also see the number of partition replicas and the idea of the cluster controller below. You can find key indicators of the glass or health for diskettes shows the maximum and the minimum disk usage on different brokers. And if they the distribution becomes too and balanced, for example, due to wrongly chosen partitions, you will see a warning here. Also, you can see if the network and requests red bulls are able to keep up with the load at the bottom. You can monitor each brokers throughput, and Layton's is observed. Overall, If any of your brokers is having problems, you will see it here and most likely be able to quickly pinpoint the source of the problem. Let's now go to the topic stop, which by default shows only the user created topics, and that's fine by us. If we go into the topics details so we can check the production and consumption rates and the replication status, the rates are not showing anything, since there are no messages flowing through the toppy. One of the cool things is that you can monitor individual messages on each topic by going into the messages Stop knows that it will only show messages that are received well, the stop is open, so let's see them appearing by also learning how to produce messages using the rest. Proxy, this is done by sending the records is the request payload and I already created an example of a payload for our trip. Intense topic? No, that here it is in adjacent for month and I have great news. We won't have to serialize it, Tobias before sending since when using the schema registry, Kafka can handle Jason messages in the same way it does proto before Afro when we want to produce some messages. The end point is the same as for fetching topics data in the previous file. The difference is that we need to send a post request with our Jason file contents. Also, it is essential to set the right content type and accept Heathers. Those may not look familiar, so let me explain. It is basically application Jason contents type here, but with additional non standard parts, which starts with VND standing for vendor. It is then followed by Kafka toe name, this known standards type, and Jason toe indicate that devalues of our messages. Will we provide it in Jason format? So, for example, we could also have proto above here. V two specifies the A P I version, and we have the standard Jason part that tells that the hope a load will be a Jason in the except Heather. We removed this message format part as this is valid only for sending the message to the proxy after running. This we received similarly, is for our Java client information about the partition to which the message has been assigned and it's offset here. It is one, since I must have produced a message before. If we go back to the messages stop in the control center, it will appear there. We can see not only its meta data, but also the key and the value going back to the main topic tub. It is possible to see current officers for each of the partitions. Note that the production rate is still not shown. Sayings the volume of data I produced. It's so low, but you can see a graph showing up, and if we click on the production tile, we can see that the message sent was actually accounted for. Let's produce a few more messages and see how we can consume them. Using the proxy for thes, we first need to create a consumer, which is done by calling the consumers and point with the name of the Consumers Group, which I said the booking service. In this example. This is a post request that requires a consumer definition. I placed it in a separate file where its name for Matthews for this realization and out off that recent properties air provided earliest means that it will start consuming from the oldest message. After that, we need to create a subscription bypassing the consumer i d in the u. R l and A. Pending the subscriptions segment. Again, it is a post request where we send the list off the topics we want to subscribe to. If successful, this should return to 04 status. How are the messages actually consumed using http? This is done by a pending the records segment to the consumer instance, you are l and sending a get request. Let's see the result first run returns nothing. But as such request is equivalent to a single pole loop execution let's call it again and receive all the messages that have been produced before we can check the consumers activity using the control center. There is the consumer stuff where our consumer groups should be present. If it isn't, you might need to refresh the page. This shows as the number of consumers in the group and number of topics being consumed. But why does it show that we are five messages behind? If all of them have been consumed, we can confirm it, please, by clicking Condi Consumer Group, which will open the nice draft with consumer locks. Well, since a cult of this http request works like a call to the PO method, we actually need one more cold toe, committed the offsets from the previous one. So let's call it again and verify the dealer goes to zero. That's so I prepared for this module. I hope that you are now confident that you can connect to Kafka with any technology using the rest proxy, and that by seeing county control center works, you get a feeling of how a monitoring solution for Kafka should more or less look like I also encourage you to play with the control centre a little more. While the trial license is still active

Summary
Course Summary
[Autogenerated] congratulations on arriving at this video. If you follow the other modules of the course, then I must admit you did a tremendous work. If you started with little or no No. Which about Kafka? Now you're for sure, Ready to work with this technology. I would like to use this last video to take a look at the material that I covered, but also tell you what was not covered, where to go from here to learn more and where to look for answers in case of problems. I started by talking about message brokers in general, presenting what is their place in the modern infrastructure on the example of a right hailing up and comparing popular patterns, namely message queue and pop sub. In the next module you started the Kafka broker for the first time. So how to interact with it by using the admin client and learned about a very important concept, a partition. At the end, you were able to send a message using your producer and receive it on the other side using a consumer. Then we zoomed in on consumers and producers. On the producer side, you learned about the partition keys, retry mechanisms budging and compression. I showed you how you can configure a producer to achieve your latent see and ______ goals. On the consumer side, I explained how officer committed how consumer groups are managed and what happens in case of producer crashes. In the next module, we focused on the central feature of Kafka, its reliability guarantees. For the first time, we deploy the cluster consistent off three brokers. And so how Kafka handles falls and guarantees high availability through its replication mechanism. I also briefly discussed how you can configure the data retention policies and how did just the right size 40 servers in the cluster after dead, I decided to focus on the structure of the messages and presented how you can handle scheme as of your messages with critical buffers and simplify your work with the confluence Chema registry, which can also handle Afro and Jason Data in a similar way. Up to this point, you learned a lot about Kafka, but we had always used pre built images, so I dedicated the module to show you how to install CAFTA and zookeeper from scratch and run multiple instances of age. Last module introduced to useful extensions from Con Fluent, the risk proxy for connecting any application with Kafka and an enterprise off a rink. The control center, which helps you monitor and manage the plaster. That's quite a lot of material again. Congratulations. I hope that now you see how having a Kafka cluster deployed in an organization can help connect the different services easily. And I have no doubt that you now have the knowledge to start working with Kafka in the rial life environment. Of course, as this is an introductory course and Kafka is a complex piece of technology, there are still many details that were not discussed in this course. Hundreds or thousands of configuration options, protocols, details, plaster replication or subtleties off error handling are just few examples off that tough guys also being actively developed. And there are cool new features added to it on the regular basis. For example, case equal and even thrilling database or Kafka streams as small but powerful toolkit for building streaming applications around Apache Kafka. So there is, without doubt a lot more that you can learn bait about core Kafka technology or about things built around it. If you are curious and want to continue developing your Kafka skills. I definitely encourage you to look for more courses on the topic in the plural site library. Also, I strongly advise getting a copy off the Kafka, the definitive guide book. It is written by the creators of Kafka, and in several places it expands on the concepts presented in this course. It is also good to have it as a reference, since it describes many low level concepts in great detail. I also strongly suggest going to the Kafka website and getting familiar with his documentation structure to be able to search it whenever needed. It is a go to place whenever you need to learn about some configuration options, and also, whenever you're looking for some implementation details in case of any issues, you should have no problems with finding help as Kafka has a large inactive community. So there is lots of activity on mailing Lease and Q A platforms, and that's it. Thank you very much for watching the course. I hope that you will enjoy working with Kafka, and I'll be happy to hear your feedback